{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da4688c",
   "metadata": {},
   "source": [
    "# Flood Mapping Example\n",
    "\n",
    "This notebook provides the code to reproduce the results of the paper for what concerns the flood data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6592b",
   "metadata": {},
   "source": [
    "## 0. Preamble\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56205c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # Delete all files and folders in the current directory\n",
    "# for item in os.listdir():\n",
    "#   item_path = os.path.join(os.getcwd(), item)\n",
    "#   if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "#     os.unlink(item_path)  # Remove file or symbolic link\n",
    "#   elif os.path.isdir(item_path):\n",
    "#     shutil.rmtree(item_path)  # Remove directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73dfbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "REPO_URL = \"https://github.com/FractalySyn/PiNets-Alignment.git\"\n",
    "FOLDER = \"floods\"\n",
    "\n",
    "if not os.path.exists(FOLDER) or True:\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"--filter=blob:none\", \"--no-checkout\", \"--sparse\", REPO_URL, \"tmp_clone\"],\n",
    "        check=True,\n",
    "    )\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"-C\", \"tmp_clone\", \"sparse-checkout\", \"set\", FOLDER], check=True)\n",
    "        subprocess.run([\"git\", \"-C\", \"tmp_clone\", \"checkout\", \"main\"], check=True)\n",
    "        shutil.copytree(os.path.join(\"tmp_clone\", FOLDER), FOLDER, dirs_exist_ok=True)\n",
    "    finally:\n",
    "        shutil.rmtree(\"tmp_clone\", ignore_errors=True)\n",
    "else:\n",
    "    print(f\"Directory '{FOLDER}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9929c0",
   "metadata": {},
   "source": [
    "The code chunk below will download the [Sen1Floods11](https://github.com/cloudtostreet/Sen1Floods11) dataset and the [Prithvi foundation model](https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-1.0-100M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  os.mkdir('floods/external_data/')\n",
    "  !gsutil -m rsync -r -x \".*(JRCWaterHand|S1OtsuLabelHand|S1Hand).*\" gs://sen1floods11/v1.1/data/flood_events/HandLabeled/ floods/external_data/\n",
    "  !gsutil -m rsync -r gs://sen1floods11/v1.1/splits/flood_handlabeled/ floods/external_data/\n",
    "  \n",
    "  os.mkdir('floods/external_data/prithvi/')\n",
    "  !git clone https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-1.0-100M floods/external_data/prithvi/\n",
    "  %pip install -q rasterio\n",
    "  \n",
    "  print('Done downloading! The data is in the floods/external_data/ directory.')\n",
    "except:\n",
    "  print('The floods/external_data/ directory already exists! Skipping download. Verify that the data is present or remove the external_data/ folder and rerun.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Derandomization and determinism setup for reproducibibility\n",
    "from floods.utils import env_reproducibility\n",
    "env_reproducibility()\n",
    "\n",
    "SEED = 2025            ## random seed for reproducibility\n",
    "\n",
    "### Pytorch engine --- uses GPU (cuda) if available\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96467d4a",
   "metadata": {},
   "source": [
    "## 1. Satellite Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DIR = 'floods/external_data/'\n",
    "\n",
    "## we make our own split so we combine train and val sets here\n",
    "train_df = pd.read_csv(DIR+'flood_train_data.csv', header=None)\n",
    "val_df = pd.read_csv(DIR+'flood_valid_data.csv', header=None)\n",
    "train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "del val_df\n",
    "\n",
    "test_df = pd.read_csv(DIR+'flood_test_data.csv', header=None)\n",
    "\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287378d",
   "metadata": {},
   "source": [
    "**Example of Sentinel-2 data and flood masks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc958e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "## plotting setup\n",
    "colors = ['black', 'lightgray', 'teal']\n",
    "cmap = mcolors.ListedColormap(colors)\n",
    "bounds = [-1.5, -0.5, 0.5, 1.5] \n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417a598",
   "metadata": {},
   "source": [
    "**Visualize a sample from the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202703d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize a sample from the test set\n",
    "row = test_df.iloc[33]\n",
    "print(row)\n",
    "\n",
    "## load Sentinel-2 image and mask\n",
    "S2_path = DIR+'S2Hand/' + row[0].replace('S1', 'S2')\n",
    "mask_path = DIR+'LabelHand/' + row[1]\n",
    "with rasterio.open(S2_path) as src:\n",
    "  S2 = src.read()\n",
    "with rasterio.open(mask_path) as src:\n",
    "  mask = src.read(1)\n",
    "\n",
    "## preprocess S2 for visualization\n",
    "S2 = S2 / 3000.0\n",
    "S2_nir = S2[8]\n",
    "S2 = S2[[3,2,1]].transpose(1,2,0)\n",
    "\n",
    "## plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14,5))\n",
    "[ax.axis('off') for ax in axs]\n",
    "\n",
    "axs[0].imshow(S2, vmax=1)\n",
    "axs[1].imshow(S2_nir, cmap='gray')\n",
    "axs[2].imshow(mask, cmap= cmap, norm=norm);\n",
    "\n",
    "axs[0].set_title('Sentinel-2 RGB bands')\n",
    "axs[1].set_title('Sentinel-2 NIR band')\n",
    "axs[2].set_title('Annotated Mask');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862ea7f",
   "metadata": {},
   "source": [
    "**Load images in memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22798ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from floods.utils import load_X, load_pi\n",
    "\n",
    "## load data (images X and masks pi*)\n",
    "X_train = np.array([load_X(row, DIR) for _, row in train_df.iterrows()])\n",
    "pi_star_train = np.array([load_pi(row, DIR) for _, row in train_df.iterrows()])\n",
    "\n",
    "X_test = np.array([load_X(row, DIR) for _, row in test_df.iterrows()])\n",
    "pi_star_test = np.array([load_pi(row, DIR) for _, row in test_df.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb60cf02",
   "metadata": {},
   "source": [
    "We downsample the 512x512 images and masks into four insets of dimensions 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from floods.utils import split_into_insets, flatten_data, train_val_split, seed_all\n",
    "\n",
    "X_train_insets = split_into_insets(X_train)  \n",
    "pi_star_train_insets = split_into_insets(pi_star_train)  \n",
    "area_train_insets = np.stack([(pi_star_train_insets==k).sum(axis=(2,3)) for k in [-1,0,1]], axis=2)\n",
    "\n",
    "X_test_insets = split_into_insets(X_test)  \n",
    "pi_star_test_insets = split_into_insets(pi_star_test)\n",
    "area_test_insets = np.stack([(pi_star_test_insets==k).sum(axis=(2,3)) for k in [-1,0,1]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flatten data\n",
    "X_train_flat, pi_star_train_flat, area_train_flat = \\\n",
    "  flatten_data(X_train_insets, pi_star_train_insets, area_train_insets)\n",
    "X_test_flat, pi_star_test_flat, area_test_flat = \\\n",
    "  flatten_data(X_test_insets, pi_star_test_insets, area_test_insets)\n",
    "\n",
    "# Train/validation split\n",
    "seed_all(SEED)\n",
    "X_train_flat, X_val_flat, pi_star_train_flat, pi_star_val_flat, area_train_flat, area_val_flat = \\\n",
    "  train_val_split(X_train_flat, pi_star_train_flat, area_train_flat, val_split=0.2)\n",
    "\n",
    "# Prepare test scenes for visualization\n",
    "S2_scenes = X_test_flat[:, [2,1,0]].permute(0,2,3,1).numpy()\n",
    "S2_scenes = (S2_scenes - S2_scenes.min()) / (S2_scenes.max() - S2_scenes.min()) * (10000 / 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343d527",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac3876",
   "metadata": {},
   "source": [
    "### 2.1. Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d203493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "## Phritvi hyperparameters\n",
    "with open(DIR+'prithvi/config.json', 'r') as file:\n",
    "  config = json.load(file)['pretrained_cfg']\n",
    "\n",
    "## modify config for our use case\n",
    "config['mask_ratio'] = 0.0\n",
    "config['img_size'] = 256\n",
    "config['depth'] = 4\n",
    "\n",
    "del config['mean'], config['std']\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65074fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32        ## batch size for training\n",
    "N_EPOCHS = 200         ## max training epochs\n",
    "lr = 5e-3              ## initial learning rate\n",
    "weight_decay = 5e-3    ## weight decay for optimizer\n",
    "patience = 1           ## lr scheduler patience\n",
    "factor = 0.2           ## lr scheduler division factor\n",
    "ES = 1e-5              ## early stopping - loss threshold\n",
    "\n",
    "load_pretrained = True   ## whether to load pretrained weights\n",
    "freeze_encoder = True    ## whether to freeze encoder weights during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('floods/models/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c57ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "seed_all(SEED)\n",
    "\n",
    "## normalization\n",
    "mu, sigma = X_train_flat.mean(dim=(0,2,3), keepdim=True), X_train_flat.std(dim=(0,2,3), keepdim=True)\n",
    "\n",
    "## Create DataLoaders for pytorch\n",
    "train_dataset = TensorDataset((X_train_flat-mu)/sigma, area_train_flat, pi_star_train_flat)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataset = TensorDataset((X_val_flat-mu)/sigma, area_val_flat, pi_star_val_flat)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = TensorDataset((X_test_flat-mu)/sigma, area_test_flat, pi_star_test_flat)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd973918",
   "metadata": {},
   "source": [
    "### 2.2. Baseline SegNet\n",
    "\n",
    "We train the baseline segmentation model with the binary cross entropy loss. Alternatively it can be trained with the false abstraction rate as a loss function. For this set `loss_='FAR'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd6a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from floods.utils import free_ram, model_size\n",
    "from floods.models import PiPrithvi\n",
    "\n",
    "free_ram()\n",
    "seed_all(SEED)\n",
    "\n",
    "## Initialize Pi-Prithvi model\n",
    "model = PiPrithvi(config, C_out=3, dim_in=config['img_size'], dim_out=256, DIR=DIR+'prithvi/',\n",
    "                  load_pretrained=load_pretrained, freeze_backbone=freeze_encoder, device=device)\n",
    "model.to(device)\n",
    "model_size(model)\n",
    "\n",
    "## Set loss and optimizer\n",
    "loss_ = 'BCE' # or FAR\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience)\n",
    "\n",
    "path = 'floods/models/segnet.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d849efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from floods.training import training\n",
    "\n",
    "## training\n",
    "model = training(model, train_loader, val_loader, optimizer, scheduler, loss_, N_EPOCHS, device, ES=ES, path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ff3e3",
   "metadata": {},
   "source": [
    "### 2.3. PiNet\n",
    "\n",
    "We train the PiNet model with the mean absolute error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_ram()\n",
    "seed_all(SEED)\n",
    "\n",
    "## Initialize Pi-Prithvi model\n",
    "model = PiPrithvi(config, C_out=3, dim_in=config['img_size'], dim_out=256, DIR=DIR+'prithvi/',\n",
    "                  load_pretrained=load_pretrained, freeze_backbone=freeze_encoder, device=device)\n",
    "model.to(device)\n",
    "model_size(model)\n",
    "\n",
    "## Set loss and optimizer\n",
    "loss_ = 'MAE'\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience)\n",
    "\n",
    "path = 'floods/models/pinet.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training\n",
    "model = training(model, train_loader, val_loader, optimizer, scheduler, loss_, N_EPOCHS, device, ES=ES, path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860707a",
   "metadata": {},
   "source": [
    "Add\n",
    "- RETRAIN\n",
    "Delete\n",
    "- first cell\n",
    "- sample (10)\n",
    "- or True\n",
    "- otsu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
